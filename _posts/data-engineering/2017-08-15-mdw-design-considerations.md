---
layout: default
title: Modern Data Warehouse Architectures - Design Considerations
description: Design considerations for modern data warehouses
categories: [data engineering, analytics]
---

## Design Considerations

### Storage

The first consideration is naturally storage. Which storage solution should we use. If your existing infrastructure is in house, then this severely restricts your choices and most likely an in-house [HDFS](http://hadoop.apache.org/) cluster, [MapR-FS](https://mapr.com/products/mapr-fs/) (see [comparison](https://mapr.com/blog/mapr-fs-vs-hdfs-5-minute-guide-understanding-their-differences-whiteboard-walkthrough/)) or similar are really the only options. We would still recommend examining the possibility of using a cloud provider. However, remember that whatever option you choose, the compute and storage **must** to be on the same network. You do not want to pull data outside of a cloud provider's network. If you are already a cloud user, then you have the choice of running a HDFS cluster in the cloud vs using that cloud providers storage solution. It is very difficult to recommend to anyone to run their own HDFS clusters in the cloud over choosing that of the cloud provider. There are so many benefits in choosing the cloud provider storage solution, from performance to elasticity, versioning, regional replication, cold storage and the cost of running. Some support 4 9’s availability and 11 9’s durability. The operational costs of running a performant HDFS cluster are extremely high and it is very difficult for most custom HDFS setups to achieve even near similar performance, availability or durability to the cloud providers. Resource isolation and starvation issues become a considerable time sink with custom HDFS clusters. 

If you are already using a cloud provider, then you are unlikely to choose another providers storage for similar reasons to in-house solutions not choosing a cloud solution. Compute and storage must be within the same network. Each cloud provider's storage has its own benefits and shortcomings vs the others and we’re not going to list them off here. Similarly a performance benchmark which is up to date is difficult to find and so we will not link to any. One should always perform their own benchmark for themselves of the given solutions they are investigating. However, from experience we have found that one can concurrently read data from cloud storage to different nodes in the same region, within the provider's network and max the 1Gbps network card on the nodes. With speeds like this, data access is not going to be the bottleneck for your analytics engine. While the Time To First Byte ([TTFB](https://en.wikipedia.org/wiki/Time_To_First_Byte)) in most cloud storage solutions will be slower than reading from a local disk or dedicated attached instance storage (e.g. YARN and HDFS with data local computation), that initial latency is negligible when working with data volumes required for analytics. The more important metric is throughput or Time To Last Byte (TTLB). We want solutions that can give us a higher throughput.  

Some early cloud storage solutions such as [AWS S3](https://aws.amazon.com/s3/ ) had certain eventual consistency quirks in the early days. However, these are well and truly resolved. All of the main providers guarantee read-after-write consistency for PUT operations on new objects, and some even for PUT operations on existing. However, all providers, including S3 now have a transparent solutions for dealing with any remaining consistency issues. The new [S3Guard](https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.1/bk_cloud-data-access/content/s3-guard.html) feature of the S3A filesystem implementation is bundled with the latest versions of [Hadoop](http://hadoop.apache.org/) (see [HADOOP-13345](https://issues.apache.org/jira/browse/HADOOP-13345)) and if working with [EMR](https://aws.amazon.com/emr/) one can use the [EMRFS](http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html). This means one can use these providers without any fear of consistency issues.

Almost all big data tools have standardised around the Hadoop file system interface for I/O. This interface abstracts away the fact that one is working against native [HDFS](http://hadoop.apache.org/), [AWS S3](https://aws.amazon.com/s3/), [Google Storage](https://cloud.google.com/storage/), [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/storage-introduction) or any other provider. This means that one is not tied into a decision they make now, and if needs or circumstances change, they can easily (in terms of interaction) change the storage solution to any of the supported implementations.

### Storage Formats

Storage format optimisations were a major factor in making a "queryable" Data Lake feasible. Columnar database solutions such as [Vertica](https://www.vertica.com) started appearing in the early 2000’s to tackle the analytics challenges of Big Data. When columnar data formats such as [ORC](https://orc.apache.org/) and [Parquet](https://parquet.apache.org/) started appearing as standalone projects, it allowed the benefits of columnar storage to be plugged into a variety of existing storage solutions. Writing the data lake directly in these formats started making more sense.

So what does columnar storage provide? At the most fundamental level, columnar storage allows one to reduce the amount of data an engine must read from storage, to only what is necessary to solve a given query. Predicate and Projection projection pushdown are crucial concepts. Unlike typical row based datastores where performing an aggregate query such as that below requires reading all of the data (every column) from every single row of the table, a columnar solution can read only the data for the rows it requires (predicate pushdown)  and only the data for the single column it requires (projection pushdown). If there are only 10 columns of equal datasize in the table, then you immediately have an order of magnitude reduction in the amount of data to read.  

<table>
  <tr>
    <td>SELECT AVG(col_a) FROM table_name WHERE col_a > 100</td>
  </tr>
</table>


A regular row based database could potentially add an index to the column so it does not need to read every row, however, if the number of records to be read is over a certain threshold, the index is ignored and a full range scan is performed anyway as it is faster than using the index. The use of an index by its nature means that reading is less sequential and there may be more random IO which is extremely detrimental to analytical queries. Indexes also require additional storage. Columnar storage not only removes the need for the storage used by indexes, it also handles sparse data and allows for optimal encoding and compression of the column values which reduces the storage costs, and the amount of data which must be read for a given query. 

Columnar storage provides many other benefits depending on the format. Parquet for instance enables one to work with highly nested structures as if they were automatically denormalized (we discuss this further below). Partition pruning is another concept which can dramatically reduce the amount of data which must be read to satisfy some queries. The storage level should support a partitioning strategy which can then be used by the query engine. Note that this feature is not unique to columnar data formats and most row based databases have supported advanced partitioning and clustering configuration for a long time. A more detailed discussion of the differences between row and column base storage can be found in this [paper](http://db.csail.mit.edu/pubs/ssbm.pdf).

#### Nested structures

In a classical data warehouse, complex relational structures are either fully denormalized or partially denormalized. The level of denormalization sometimes depends on the level of redundancy and complexity one can manage in a given table. It also depends on the depth of the related entities as in the case of a 1:n relationship, clearly we cannot denormalize past a certain threshold. To query such a denormalized structure can be very complex and lead to the creation of multiple views of the same data, denormalized in different ways to work for particular query patterns. 

To take the classic example of representing an order and its order items from a purchase. The logical structure is pretty simple: an order can have one or more order items. In an [OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing) system this would be modeled as two different tables: one for order items and one for orders with referential constraints between the two table. In a [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing) system the same object would be modeled using a star schema, by denormalizing the OLTP version into a order items fact table and create dimension for each attribute of the order. We all have done at least once in this way. This model has various shortcomings: the first one appears when you want to join other object together. For example, in this case if you are looking at attributing orders to specific events, you will have the classic fan-out problem. Things get even messier if you need to handle updates to orders, because you will need to either do a full delete / insert of the order or update the fields that have changed. The order and order item may also have other child entities such as consumers or segments in the travel industry. The number of tables and the relationships between them can get out of control very quickly. The complexity can become even worse if you are trying to do this with an SQL style [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load).

Managing the denormalization comes at a severe cost. The ETL complexity, the modelling complexity, the maintenance complexity. Anyone who has been on the receiving end of keeping a data warehouse consistent when updates to denormalized entities can occur knows that this is something we should try to avoid at all costs. Very often the understanding of the data model is completely lost and what was received or consumed by the original endpoint is all but indistinguishable when it comes to querying at the analytics end. This leads to additional complexity and time wasted from both a business analysis and data science perspective. If using a classical OLAP database system such as Redshift, you have no choice but to go down this route if you want performance. 

Wouldn’t it be nice to just add the order entity, as-is to the storage, and query it directly as if it had been denormalized? This is exactly what Parquet allows you to achieve and it does so without any performance penalty. In fact, in many if not all use cases, there is an improvement in performance due to the removal of the physical JOINs between the various parent-child entity relationships. With Parquet, the Order and all of its child entities are stored as a logical record in an "orders" Parquet files (logical table). Within these files, the data is flattened and stored in columnar format, taking care of nested, repeated elements and allowing optimal storage and query efficiency. We will discuss the Parquet storage format in more detail in a subsequent post in this series, but if you wish to know more know, read or watch one of the presentations available on the parquet [website](https://parquet.apache.org/presentations/).

#### Real-Time Reporting

Many organisations require real-time visibility of certain key metrics. Creating a unified query system for both real-time data and historic is an extremely difficult task. Most companies will accept a reasonable ETL delay of minutes to hours for some dashboards to update. In fact, the majority of business metrics do not need to be real-time, despite what you may hear. One should always evaluate what metrics really require real-time visibility and what that definition of real-time is. 

Metrics which only keep track of some recent information, e.g. a sliding window of the top N sites visited in the last 30 minutes are much easier to achieve than metrics which require a real time view of information which also has a long term historical presence, e.g. total purchases of a given product over all time. The former can be solved via a solution designed only for real-time time series as it doesn’t need to worry about long term storage. It can optimise for such queries easily and there are many databases which can achieve this with little effort. It can also be solved via approximate counting algorithms of which there are many that give an accuracy to within 1% and require minimal resources to maintain. When 100% accuracy is not required and / or scale and latency requirements prevent it, approximate algorithms should be used due to their simplicity and cost effectiveness. 

Unifying a query interface for a metric which has both a real-time and a historic component is much more difficult. [Druid](http://druid.io/) is probably the most well know OLAP system which attempts to do so. It essentially has separate processes, data structures and storage for real time and historic data, but it unifies the query behind a single data interface. It allows for very high performance and low latency queries on real-time and historic data by keeping approximates and rollups of the data. A common critique of Druid is that it's difficult to set up and manage.  There is a reason for this and it is the inherently different properties of the systems required to meet both the real-time and historic queries in a performant manner. In our opinion, there is currently no avoiding such a setup and any systems that have tried to unify the two concerns have failed horribly. A critique that we do have of Druid however is its lack of ANSI SQL support. It has its own query language which is not something we feel belongs in a modern Big Data stack and for all of the reasons we mentioned in the ANSI SQL section, it is not something we recommend. There is support for certain SQL features via [Apache Calcite](https://calcite.apache.org/) but this is marked as experimental. However, depending on your reporting requirements, it may be simpler to use than developing and managing the real-time aggregates and merging them with the historic results via a custom solution. [Pinot](https://github.com/linkedin/pinot/wiki) from LinkedIn is another real-time OLAP and shares many of the same design features, benefits and shortcomings as Druid. Naturally they have certain differences which should be analysed in more detail before choosing one over the other. 

It is worth mentioning [MemSQL](MemSQL) in this section, as it appears like an obvious solution when real-time analytics is required and costs are not such an issue. It is a closed source product which requires licensing to use basic enterprise features such as high availability. That said, it is a fully relational SQL engine (i.e. ACID transactions), supports streaming ingestion of data and works at very high scale. These are not features one finds in many (or any) existing database solutions. [VoltDB](https://www.voltdb.com/) is another solution with some similarities to MemSQL and is founded by a true pioneer in the database world, Michael Stonebraker. However, it is intended more for the OLTP world with real-time analytics capabilities on recent data, whereas MemSQL attempts to fit both the OLTP and OLAP worlds (it supports row based and column based storage). VoltDB recommends performing batch analytics against another Data-mart or Hadoop environment (see [here](https://www.voltdb.com/why-voltdb/real-time-analytics/)) which means two distinct systems again. If you require a fully relational engine with ACID transactions and wish to have all of your eggs in one basket (single storage engine for [OLTP](https://en.wikipedia.org/wiki/Online_transaction_processing) and OLAP) then you could look at MemSQL. It does however go against many of the principals we put forward in this article. 

Finally, there is an interesting project called [Apache Kudu](https://kudu.apache.org/) which is an apache incubator at the time of writing. This has the potential to enable real-time querying of changelog (journal) data which could then be merged with the results of a historic query. [Impala](https://impala.apache.org/) currently supports querying of Kudu and other engines should do so shortly. Using SQL windowing it is then possible to effectively merge the real-time and historic data to get an up to date value. This concept is still in its infancy and is more complex to manage than is recommended. However, the space and concept could evolve quickly. 

### Resource Isolation, QoS and Multi-tenancy

Resource allocation, isolation and QoS are probably one of the most overlooked area in data engineering. While auto scaling resources and tooling has helped to reduce this burden in recent years, it is still extremely difficult to manage across the four critical contention areas of cpu, memory, network and disk. Many users tend to forget that often, they are not the only users of the data warehouse and run complex queries which can have an impact on other processes and users. Maybe they forget to notify an administrator or another group within the company when running a large job and they end up impacting latencies, or worse, taking down a critical service or datastore. If this happens, who’s fault is it? We would argue that it is the architectures fault and not the users. Sufficient resource isolation should be in place via multi-tenancy support. However, providing accurate and sufficient resource isolation in most environments is extremely difficult. YARN has support for cpu and memory isolation and extremely basic support for disk isolation. However the necessary disk and network isolation (bandwidth allocation etc) are not currently possible with YARN. There are open tickets to tackle these issues and work is ongoing (see [YARN-2139](https://issues.apache.org/jira/browse/YARN-2139) and [YARN-2140](https://issues.apache.org/jira/browse/YARN-2140)). Add HDFS disk and network isolation issues into the mix and the complexity is not possible to configure and manage effectively. 

This is where isolated environments tend to be a better fit than shared environments. This isolation needs to start from the underlying data access to the query execution. When both your data and query execution engines are a single unit (e.g. [Redshift](https://aws.amazon.com/redshift/)), this type of isolation is not possible unless strictly supported by the product. However, even then it is difficult for the product to guarantee this from the storage throughput, network, memory and cpu perspective. When the data and engine are separated, there are a range of opportunities and flexible options one can take to ensure isolation without being constrained to a particular solution. In cloud environments this can save considerable money while dramatically increasing flexibility. When more execution resources are needed it is easy to scale up a cluster and scale it back down when complete. This resource elasticity is typically available within a few minutes of the request for the resource. In a classical OLAP configuration, adding a new node is a serious consideration and not temporary. It can take days or more for a new node to fully join a cluster while all the data rebalancing takes place. During this time the cluster is under severe strain and the existing performance is degraded. 

The ability to have isolated execution environments for different users or groups becomes trivial when the storage is separated. That might be different tenants of different organisational departments. Forget about resource contention or starvation and simplify the breakdown of billing and costs to individual users. Take the example of having your data stored on S3. You can chose to start a new [EMR](https://aws.amazon.com/emr/) cluster with either [Spark](https://spark.apache.org/), [Presto](https://prestodb.io/), [Hive](https://hive.apache.org/) or combinations thereof and query the data without any fear of affecting another team or user running queries against the same data in another cluster. This also allows for easy cost allocation as the costs associated with running a given instance of a cluster can be easily associated with a given user or group. 

Of course the storage must be capable of supporting concurrent users, and scaling to meet the demand. As we discussed already, cloud storage solutions such as S3 are very difficult to beat. When necessary, setting up bucket and cross regional replication is trivial and ensures that users can work with data where and how they want. Data can be temporarily copied and deleted with minimal charges to meet any scaling demands (copying from S3 to S3 is free in the same region). 

### Query Engines

#### Overview

As we have mentioned numerous times in this article, the goal of the data architecture we discuss in this post is to remove the burden of having to pick a single query engine or analytics tool. We need the ability to switch between engines as the use case arises and technology advances. If engine A isn’t working well for this particular workload and engine B is better suited, with the storage and query concerns separated, it’s easy switch or use both. When the storage and query engine are collocated (such as [Vertica](Vertica), [Oracle](https://www.oracle.com), etc) it becomes much more difficult and orders of magnitude more time consuming and expensive to test alternative query engines and is typically not even considered an option. This severely restricts business flexibility. 

There are a baffling number of database solutions out there. It is impossible to give a thorough review of each of them or say which is the best for your particular use case. This said, we will provide an overview of some points to be aware of when looking at a suitable engine and discuss use cases both approaches may be suitable for. 

#### Multi Data Source Support

While the general goal of a Data Lake is to contain all of your data in one location, there are times when some data such as application metadata or inventories are located in different systems or storage. The ability to connect to these disparate systems and storage and join with our main data warehouse in very valuable. Take something as simple as the metadata for an Organisation identifier. Instead of maintaining a copy of this data in the data lake, some tools allow reading directly from the metastore which holds this data and joining with behavioural or transactional data stored in the Data Lake. This completely removes some pointless ETL tasks and data synchronization issues. The data source may even be another S3 bucket. Tools such as [Spark](https://spark.apache.org/), [Drill](https://drill.apache.org/), [Impala](https://impala.apache.org/) and [Presto](https://prestodb.io/) support many data sources. In many organisations, such support can remove thousands of unnecessary ETL tasks and synchronization headaches. 

Naturally one must be careful with an analytics engine talking directly to an application database, and the correct controls should be in place. However, correct usage means there is no more strain placed on the database than an ETL job to extract changes. We are talking about application metadata here, not transactional or behavioural data and so the read volumes are small (potentially to the extent that it should constitute a broadcast join in most engines). 

#### Schema Support

Up front schema modelling of data warehouse solutions is no longer a mandatory task. Many tools are built to infer the schema from the data sources they are pointed to. Drill in particular is built with this in mind and is marketed as "schema free SQL". This means it can infer the schema by either analysing a sample of the files (such as json), or by examining some other schema information collocated with the data files (such as parquet, avro, orc schemas etc). Spark and many other tools can perform similar schema inference and merging. 

When it comes to analytics, we feel that the data format used should be schema based. Most efficient data formats require a schema and all columnar formats we are aware of require one. However these schemas, and how they evolve are very different to a traditional schema design for a data warehouse application. There is no need for a team of business analysts to spend weeks, months or years designing a data model **up-front**, which analysis has shown will suffice for all requirements. Many times the analysis is out of date before it has been implemented. This form of up-front schema modelling is not scalable and should simply be forgotten about. We do not want 6 month data modelling and new ETL process turnaround times for new use cases. 

As some storage formats we have discussed allow for storing **all **the data received using the same schema it was received with, there is no need for a separate schema modelling task. The only conversion that takes place is related to the storage format. For instance, if an upstream system is sending order records in avro data format, we would ensure that when we ingest the records into the data lake, they are written in parquet format, but with the same schema as the avro record. Similarly, if an upstream system was sending JSON records, we would write them in parquet and ensure we produced the corresponding parquet schema to match (extra care is required when dealing with JSON as backward incompatible changes can be introduced and more strict schema evolution and validation procedure should be employed). As the schema for the parquet files is actually written alongside the data files, we have an automated schema management solution also. This collocation of the schema and data is important. 

The benefits of this approach should be apparent. We do not require expensive schema analysis and the schema management process is essentially automated. We have all of the data we received stored and available to query. That means we are never "missing" an attribute when we look to deliver a new use case and can skip the dreaded process of “adding” new attributes to our data warehouse. New attributes and entities all naturally flow from their sources. This does of course mean that validation at source is important in many cases, as we are trusting that new attributes we encounter are to be automatically added and queryable in our data warehouse. Hence, it is desirable to have schema validation performed at the point of capture (API) where possible and feasible. Where not feasible, adequate automation rules should be in place to deal with nonconforming data (e.g. where an existing attribute changes data type etc). 

A very good read on this topic and others which one should be aware of called "The Seven Tenets of Data Unification" from Michael Stonebraker is available [here](https://www.tamr.com/landing-pages/dr-michael-stonebrakers-7-tenets-data-unification-3/). Section 2.2 is especially applicable to what we have just discussed.

#### Nested Structures

We have discussed some of the benefits of nested structures [already](#heading=h.auhzgk6o6pdk). We must be able to then query this data. The first time we came across the concept of querying nested structures within an SQL environment was when using [BigQuery](https://cloud.google.com/bigquery/). While slightly confusing at the beginning, we noticed how much safer from a data lineage perspective they are when compared to more classical data storage patterns, where the data belonging to a single logical entity is spread across multiple entity tables. There are varying levels of support and varying syntax for working with and "unflattening" such nested structures. In particular for dealing with nested arrays of structures (e.g. order items). Hive and Spark call this an “explode” operation while other engines such as Presto and Impala call it an “unnest” operation. 

One can then easily create a "view" in the given query engine to provide access to the child entity as if it was denormalized into its own table. Taking the order items example

<table>
  <tr>
    <td>CREATE VIEW orderItems AS 
             SELECT T.* FROM orders CROSS JOIN UNNEST(orderItems) AS T</td>
  </tr>
</table>


From an analytics perspective there are some important steps to consider when querying nested data structures which we have covered in a previous [post](https://docs.google.com/document/d/1v2X0JqA66vCJUBRmCW3coFya2oSGe5G4C-ioEqQfWoU/edit). 

One additional point to be aware of with Parquet format and nested structures, is that not all query engines support projection and predicate pushdown past the first level of nesting. This is an implementation detail and open issue with both [Presto](https://github.com/prestodb/presto/issues/2508) and [Spark](https://issues.apache.org/jira/browse/SPARK-4502) [a](https://issues.apache.org/jira/browse/SPARK-4502)n[d](https://issues.apache.org/jira/browse/SPARK-4502) [s](https://issues.apache.org/jira/browse/SPARK-4502)h[o](https://issues.apache.org/jira/browse/SPARK-4502)u[l](https://issues.apache.org/jira/browse/SPARK-4502)d[ ](https://issues.apache.org/jira/browse/SPARK-4502)b[e](https://issues.apache.org/jira/browse/SPARK-4502) [f](https://issues.apache.org/jira/browse/SPARK-4502)i[x](https://issues.apache.org/jira/browse/SPARK-4502)e[d](https://issues.apache.org/jira/browse/SPARK-4502) [s](https://issues.apache.org/jira/browse/SPARK-4502)h[o](https://issues.apache.org/jira/browse/SPARK-4502)r[t](https://issues.apache.org/jira/browse/SPARK-4502)l[y](https://issues.apache.org/jira/browse/SPARK-4502) [i](https://issues.apache.org/jira/browse/SPARK-4502)f[ ](https://issues.apache.org/jira/browse/SPARK-4502)t[h](https://issues.apache.org/jira/browse/SPARK-4502)e[y](https://issues.apache.org/jira/browse/SPARK-4502) [a](https://issues.apache.org/jira/browse/SPARK-4502)r[e](https://issues.apache.org/jira/browse/SPARK-4502) [n](https://issues.apache.org/jira/browse/SPARK-4502)o[t](https://issues.apache.org/jira/browse/SPARK-4502) [a](https://issues.apache.org/jira/browse/SPARK-4502)l[r](https://issues.apache.org/jira/browse/SPARK-4502)e[a](https://issues.apache.org/jira/browse/SPARK-4502)d[y](https://issues.apache.org/jira/browse/SPARK-4502). However, Presto supports predicate push down on nested items while Spark does not as of the time of writing. As with everything, check the support and validate it with the given version of a tool you are thinking of using. 

#### ANSI SQL

Replicability of the analysis over time is a key factor in data science. As data scientists, we want to be able to rerun the sample logic on the same data structure, on a different database engine, with no change if possible.

Given the current trend towards separating the storage from the query engine, using standard ANSI SQL becomes even more important. It allows for switching from one query engine to another by simply launching the respective cluster. As there is no need to move any data into the cluster as both can query it where it is stored, such as on S3, we can immediately run the same queries on both engines. The power of this cannot be underestimated. Different query and execution engines will have different strengths and this ability to switch between is priceless. It also reduces lock in over time which can make changing engine extremely expensive, quite often prohibitively expensive in the analytics space.

Most engines have additional support for different types of SQL based analysis than that supported by pure ANSI SQL. While these can be useful for certain use cases, we feel their usage should be kept to a minimum. One clear exception to that is the UNNEST statement which is used to work with nested structures. In Spark and Hive this is called LATERAL VIEW EXPLODE. Of course there are many ANSI SQL standards from SQL-86 to SQL-16. Which level you will need to look out for is based on your requirements but at a minimum one should look for SQL-03 when window functions were added to the standard.

#### Query Isolation

Some engines such as Spark are not designed for managing multiple concurrent queries. Apart from resulting in very poor concurrent query performance, it also means they have some rather nasty failure semantics for rogue queries. A rogue Spark query will take down the entire cluster. Basically there is no isolation. Considering the typical use case for Spark is ETL this makes sense, but for an SQL engine it is a rather unpleasant feature. Contrast that to engines such as Presto where the isolation is on the individual query engine and rogue queries are killed. 

